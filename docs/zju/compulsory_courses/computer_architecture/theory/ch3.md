# 3 Memory Hierarchy Design

!!! tip "说明"

    本文档正在更新中……

!!! info "说明"

    1. 本文档仅涉及部分内容，仅可用于复习重点知识
    2. 本文档内容对应课本 Chapter 2 和 Appendix B

## 1 引言

memory hierarchy（存储器层次结构）：

principle of locality of reference（局部性原理）：程序在任意时刻通常只访问地址空间的一小部分。为了提高访问速度，可以将最近访问的数据项保存在快速存储器中。这通常通过使用缓存（cache）来实现，缓存是一种小而快速的存储器，用于存储最近或频繁访问的数据，以减少访问较慢的主存储器的次数
      1. temporal locality（时间局部性）：最近访问过的数据很可能在不久的将来再次被访问
      2. spatial locality（空间局部性）：如果一个数据被访问，那么它附近的数据也可能很快被访问

### 1.1 4 个存储器层次结构问题

如果处理器需要的数据存放在高层存储器中的某个块中，则称为一次 **命中**（hit）。如果在高层存储器中没有找到所需的数据，这次数据请求则称为一次 **缺失**（miss）。随后访问低层存储器来寻找包含所需数据的那一块。**命中率**（hit rate），或命中比率（hit ratio），是在高层存储器中找到数据的存储访问比例，通常被当成存储器层次结构性能的一个衡量标准。**缺失率**（miss rate）（1－命中率）则是数据在高层存储器中没有找到的存储访问比例

追求高性能是我们使用存储器层次结构的主要目的，因而命中和缺失的执行时间就显得尤为重要。**命中时间**（hit time）是指访问存储器层次结构中的高层存储器所需要的时间，包括了判断当前访问是命中还是缺失所需的时间。**缺失代价**（miss penalty）是将相应的块从低层存储器替换到高层存储器中，以及将该信息块传送给处理器的时间之和，由于较高存储层次容量较小并且使用了快速的存储器部件，因此比起对存储层次中较低层的访问，命中时间要少得多，这也是缺失代价的主要组成部分

#### 1.1.1 Block Placement

**一个块可以放在上一级的什么位置**

在 cache 中为主存中每个字分配一个位置的最简单方法就是根据这个字的主存地址进行分配，这种 cache 结构称为 **直接映射**（direct mapped）。每个主存地址对应到 cache 中一个确定的地址

$$
（块地址） mod （cache 中的块数）
$$

**全相联：**（fully associative） 一个块可以被放置在 cache 中的任何位置

**组相联：**（set associative） 每个块可被放置的位置数是固定的（至少两个）

每个块有 n 个位置可放的 cache 被称作 n 路组相联 cache（n-way set-associative cache）。一个 n 路组相联 cache 由很多个组构成，每个组有 n 块。根据索引域，存储器中的每个块对应到 cache 中唯一的组，并且可以放在这个组中的任何一个位置上

在组相联 cache 中，包含存储块的组是这样给出的：

$(Block\ number) mod (number\ of\ sets\ in\ the\ cache)$<br/>
$（块号）mod（cache 中的组数）$

<figure markdown="span">
    ![Img 1](../../../../img/comp_arch/ch3/ca_ch3_img1.png){ width="600" }
</figure>

#### 1.1.2 Block Identification

**如果一个块在上一级中，如何找到它**

由于 cache 中每个位置可能对应于主存中多个不同的地址，我们如何知道 cache 中的数据项是否是所请求的字呢？即如何知道所请求的字是否在 cache 中？我们可以在 cache 中增加一组 **标记**（tag），标记中包含了地址信息，这些地址信息可以用来判断 cache 中的字是否就是所请求的字。标记只需包含地址的高位，也就是没有用来检索 cache 的那些位。例如，在上图中，标记位只需使用 5 位地址中的高 2 位，地址低 3 位的索引域则用来选择 cache 中的块

我们还需要一种方法来判断 cache 块中确实没有包含有效信息。例如，当一个处理器启动时，cache 中没有数据，标记域中的值没有意义。甚至在执行了一些指令后，cache 中的一些块依然为空。因此，在 cache 中，这些块的标记应该被忽略。最常用的方法就是增加一个 **有效位**（valid bit）来标识一个块是否含有一个有效地址。如果该位没有被设置，则不能使用该块中的内容

#### 1.1.3 Block Replacement

**在缺失时应当替换哪个块**

1. random replacement：随机替换
2. least-recently used（LRU）：最近最少的被替换
3. first in, first out（FIFO）：先进先出法则

#### 1.1.4 Write Strategy

**在写入时会发生什么**

当数据写入 cache 时，数据如何写入主存？

1. **write-through**（写直达法）：将数据同时写入主存和 cache 中。但这样会花费大量的时间，一种解决方法是 write-buffer（写缓冲），当一个数据在等待写入主存时，先将它放入写缓冲中
2. **write-back**（写回机制）：新值仅仅被写入 cache 块中，只有当修改过的块被替换时才需要写到较低层次存储结构中
      1. 需要 valid 和 dirty 位
        - dirty bit 用于标记缓存行（cache line）中的数据是否已被修改但尚未写回到主存中
        - 当一个缓存行中的数据被处理器修改时，dirty bit 会被设置为“1”，表示该缓存行中的数据与主存中的数据不一致（即“脏”）
        - 如果 dirty bit 为“0”，则表示缓存行中的数据与主存中的数据一致（即“干净”）
      2. 当某个块需要被替换时，系统会检查 dirty bit
        - 如果 dirty bit 为“1”，则将该缓存行中的数据写回主存，以确保数据一致性
        - 如果 dirty bit 为“0”，则直接丢弃该缓存行，因为主存中已经存在相同的数据

优点：

1. 写直达
      1. 更容易实现
      2. 下一级存储器中拥有数据的最新副本，从而简化了数据一致性
2. 写回
      1. 写入操作的速度与缓存存储器的速度相同
      2. 使用的存储器带宽较少，使写回策略对多处理器更具吸引力
      3. 写回对存储器层次结构其余部分及存储器互连的使用少于直写，节省功耗，对于嵌入式应用极具吸引力

处理器在直写期间必须等待写入操作的完成，则称该处理器处于写入停顿（write stall）状态。减少此停顿的常见优化方法时写入缓冲区（write buffer），但即使有了此缓冲区，write stall 也可能会发生

当发生写入缺失时（cache 里没有处理器要修改的那个数据地址），如何处理？

1. **write allocate**：将目标数据块从主存加载到缓存，随后在缓存中完成写入。适用于后续可能频繁访问该数据块的场景
2. **write around** / **no-write allocate**：不将数据块加载到缓存，直接写入主存

一般搭配：

1. write-back + write-allocate
2. write-through + write-around

<figure markdown="span">
    ![Img 2](../../../../img/comp_arch/ch3/ca_ch3_img2.png){ width="600" }
</figure>

> 好家伙，计组没搞懂的东西，终于在这里搞懂了

### 1.2 缓存架构

1. unified cache（统一缓存）：所有内存请求（包括指令和数据）都通过单一的缓存进行处理
      1. 硬件需求较少，设计相对简单
      2. 由于指令和数据共享同一个缓存，可能导致缓存争用，从而降低命中率
2. spilt I & D cache（分离缓存）：使用独立的缓存分别处理指令和数据。指令缓存（I-Cache）和数据缓存（D-Cache）分开
      1. 可以减少指令和数据之间的争用，提高命中率。指令缓存通常是只读的，这简化了设计
      2. 需要更多的硬件资源

<figure markdown="span">
    ![Img 3](../../../../img/comp_arch/ch3/ca_ch3_img3.png){ width="600" }
</figure>

## 2 Cache Performance

$CPU\ time = (CPU\ execution\ clock\ cycles + Memory\text{-}stall\ clock\ cycles) \times Clock\ cycle\ time$

$Memory\ stall\ cycles = IC\times Mem\ refs\ per\ instruction（每条指令的内存访问次数）\times Miss\ rate\times Miss\ penalty$

$\Rightarrow$

$CPU\ time = IC\times (CPI + \dfrac{MemAccess}{Inst} \times miss\ rate \times miss\ penalty) \times cycle\ time$

$\Rightarrow$

$CPU\ time = IC\times (CPI + \dfrac{MemMisses}{Inst} \times miss\ penalty) \times cycle\ time$

### 2.1 AMAT

存储器平均访问时间（average memory access time）

$AMAT = hit\ time + miss\ rate \times miss\ penalty$

$CPU\ time = IC\times (\dfrac{AluOps}{Inst}\times CPI_{AluOps} + \dfrac{MemAccess}{Inst} \times AMAT) \times cycle\ time$

<figure markdown="span">
    ![Img 4](../../../../img/comp_arch/ch3/ca_ch3_img4.png){ width="600" }
</figure>

---

<figure markdown="span">
    ![Img 5](../../../../img/comp_arch/ch3/ca_ch3_img5.png){ width="600" }
</figure>

---

<figure markdown="span">
    ![Img 6](../../../../img/comp_arch/ch3/ca_ch3_img6.png){ width="600" }
</figure>

> 课本原图有误，本文档已修正

---

<figure markdown="span">
    ![Img 7](../../../../img/comp_arch/ch3/ca_ch3_img7.png){ width="600" }
</figure>

如上例所示，缓存特性可能会对性能产生巨大影响。此外，对于低 CPI、高时钟频率的处理器，缓存缺失会产生双重影响

1. $CPI_{execution}$ 越低，固定数目的缓存缺失时钟周期产生的相对影响越高
2. 在计算 CPI 时，一次缺失的缓存代价是以处理器时钟周期进行计算的。因此，即使两个计算机的存储器层次结构相同，时钟频率较高的处理器在每次缺失时会占用较多的时钟周期，CPI 的存储器部分也相应较高

<figure markdown="span">
    ![Img 8](../../../../img/comp_arch/ch3/ca_ch3_img8.png){ width="600" }
</figure>

### 2.2 miss penalty and Out-of-Order Execution Processors

**缺失代价与乱序执行处理器**

对于乱序执行处理器，如何定义“缺失代价”呢？是存储器缺失的全部延迟，还是仅考虑处理器必须停顿时的“暴露”延迟或无重叠延迟？对于那些在完成数据缺失之前必须停顿的处理器，不存在这一问题

重新定义存储器停顿，得到 miss penalty 的一种新定义，将其表示为 non-overlapped latency

$\dfrac{memory stall cycles}{instruction} = \dfrac{misses}{instruction} \times (total miss latency - overlapped miss latency)$

与此类似，由于一些乱序处理器会拉长命中时间，所以性能公式的这一部分可以除以总命中延迟减去重叠命中延迟之差。可以对这一公式进一步扩展，将总缺失延迟分解为没有争用时的延迟和因为争用导致的延迟，以考虑乱序处理器中的存储器资源。我们仅关注缺失延迟

1. length of memory latency（存储器延迟长度）：在乱序处理器中如何确定存储器操作的起止时刻
2. length of latency overlap（延迟重叠的长度）：如何确定与处理器相重叠的起始时刻（或者说，在什么时刻我们说存储器操作使处理器停顿）

由于乱序执行处理器的复杂性，所以不存在单一的准确定义

由于在流水线退出阶段只能看到已提交的操作，所以我们说：如果处理器在一个时钟周期内没有退出（retire）最大可能数目的指令，它就在该时钟周期内停顿。我们将这一停顿记在第一条未退出指令的账上。这一定义绝不像看上去那么简单。例如，为缩短特定停顿时间而应用某一种优化，并不一定总能缩短执行时间，这是因为此时可能会暴露出另一种类型的停顿（原本隐藏在所关注的停顿背后）

关于延迟，我们可以从存储器指令在指令窗口中排队的时刻开始测量，也可以从生成地址的时刻开始，还可以从指令被实际发送给存储器系统的时刻开始。只要保持一致，任何一种选项都是可以的

<figure markdown="span">
    ![Img 9](../../../../img/comp_arch/ch3/ca_ch3_img9.png){ width="600" }
</figure>

## 3 Optimizations of Cache Performance

1. Reduce the time to hit in the cache：缩短命中时间
      1. small and simple first-level caches
      2. wat prediction
      3. avoiding address translation during index of the cache
      4. trace caches
2. Increase cache bandwidth：增加缓存带宽
      1. pipelined access
      2. nonblocking cache
      3. multibanked caches
      3. multibanked caches
3. Reduce the miss penalty：降低缺失代价
      1. multilevel caches
      2. critical word first and early restart
      3. giving priority to read misses over writes
      4. merging write buffer
4. Reduce the miss rate：降低缺失率
      1. larger block size
      2. larger caches
      3. higher associativity
      4. way prediction and pseudo-associative cache
      5. compiler optimizations
5. Reduce the miss penalty and miss rate via parallelism：通过并行降低缺失代价或缺失率
      1. hardware prefetching of instruction and data
      2. compiler-controlled prefetching

### 3.1 Reduce the time to hit

#### 3.1.1 Small and Simple First-Level Caches

**to reduce hit time and power**

**小而简单的第一级缓存，用以缩短命中时间、降低功率**

1. 实现缓存所需的硬件越少，硬件中的关键路径就越短
2. 直接映射缓存比组相联缓存在读写操作上更快
3. 将缓存与 CPU 集成在同一芯片上对实现快速访问时间也非常重要

<figure markdown="span">
    ![Img 10](../../../../img/comp_arch/ch3/ca_ch3_img10.png){ width="600" }
</figure>

#### 3.1.2 Way Prediction

**to reduce hie time**

**采用路预测以缩短命中时间**

这是另外一种可以减少冲突缺失，同时又能保持直接映射缓存命中速度的方法。在路预测技术中，缓存中另外保存了一些位，用于预测下一次缓存访问组中的路或块。这种预测意味着尽早设定多工选择器，以选择所需要的块，在与缓存数据读取并行的时钟周期内，只执行一次标签比较。如果缺失，则会在下一个时钟周期中查看其他块，以找出匹配项

在一个缓存的每个块中都添加块预测位。根据这些位选定要在下一次缓存访问中优先尝试哪些块。如果预测正确，则缓存访问延迟就等于这一快速命中时间。如果预测错误，则尝试其他块，改变路预测器，延迟会增加一个时钟周期

#### 3.1.3 Avoiding Address Translation During Indexing of the Cache

**to reduce hit time**

**避免在索引缓存期间进行地址转换，以缩短命中时间**

即使一个小而简单的缓存也必须能够将来自处理器的虚拟地址转换为用以访问存储器的物理地址

为了加快这一翻译过程，我们可以使用 TLB

<figure markdown="span">
    ![Img 11](../../../../img/comp_arch/ch3/ca_ch3_img11.png){ width="600" }
</figure>

但是

1. TLB 访问延迟：即使 TLB 命中，也需要至少 1 个时钟周期完成地址转换
2. 串行依赖：缓存访问必须等待 TLB 转换完成，导致流水线停顿

为了减少这种延迟，可以采用以下优化方法

**1.虚拟索引缓存**（virtually indexed \ VI）

- 原理：直接使用虚拟地址的一部分作为缓存的索引（Index），而不依赖物理地址
- 优点：缓存访问和 TLB 转换可以并行进行（减少串行依赖）
- 限制
      - 必须确保不同虚拟地址映射到同一物理地址时不会冲突（即避免别名问题，Aliasing）
      - 通常要求缓存大小 ≤ 页大小 × 相联度

**2.物理标记缓存**（physically tagged \ PT）

- 原理：虽然索引使用虚拟地址，但缓存行的标记（Tag）仍然使用物理地址，以确保数据一致性
- 工作流程
      1. 并行操作：虚拟索引直接访问缓存，同时 TLB 转换物理地址
      2. 比较 Tag：缓存返回数据后，用 TLB 转换的物理地址与缓存行的 Tag 比较，确认是否命中
- 优点
      1. 仍然支持多进程共享缓存（因为 Tag 是物理地址）
      2. 减少关键路径延迟（TLB 和缓存访问部分并行）

**3.混合方案：VIPT**（virtually indexed，physically tagged）

- 索引（Index）：使用虚拟地址低位
- 标记（Tag）：使用物理地址（避免别名问题）
- 优点
      1. 几乎消除 TLB 对缓存访问的延迟影响（TLB 和缓存访问并行）
      2. 兼容多进程环境（不同进程的相同虚拟地址不会错误命中）

---

在每次切换进程时，虚拟地址会指向不同的物理地址，需要对缓存进行刷新。一种解决方案是增大缓存地址标志的宽度，增加一个进程识别符标志（process identifier tag / PID）。如果操作系统将这些标志指定给进程，那么只需要在 PID 被回收时才刷新缓存。也就是说，PID 可以区分缓存中的数据是不是为此这个程序准备的

!!! note "AI 解释"

    **1.问题背景：进程切换与缓存刷新**
    
    - **虚拟地址（VA） vs. 物理地址（PA）**：现代操作系统使用虚拟内存机制，每个进程拥有独立的虚拟地址空间，但最终访问的是共享的物理内存
          - **进程切换时**，相同的虚拟地址可能映射到不同的物理地址（例如，进程 A 的 `VA=0x1000` → `PA=0x2000`，进程 B 的 `VA=0x1000` → `PA=0x3000`）
          - **缓存冲突风险**：如果缓存仅用虚拟地址或物理地址索引，新进程可能错误命中旧进程的缓存数据（**别名问题**）。 
    - **传统解决方案：缓存刷新（Flush）**：在进程切换时，清空缓存（如 TLB 刷新、缓存无效化），但会导致性能损失
          - **冷启动延迟**：新进程的缓存初始命中率低，需重新加载数据
          - **频繁切换开销大**（如多任务、多线程环境）
    
    ---
    
    **2.解决方案：扩展缓存标记位，增加 PID 标识符**
    
    **(1) 核心思想**
    
    - **在缓存行的标记（Tag）中增加进程标识符（PID）**：  
          - 缓存不仅存储物理地址 Tag，还存储当前占用该缓存行的进程 PID
          - **访问缓存时**，需同时匹配 **物理地址 Tag + PID**，才能判定命中
    
    **(2) 工作流程**

    1. **缓存存储结构**  
          - 每个缓存行的标记（Tag）扩展为：`| Physical Address Tag | PID | Valid Bit |`
          - **PID 由操作系统分配**（如进程 ID 或地址空间 ID, ASID）
    2. **缓存访问过程**  
          - CPU 访问虚拟地址 → TLB 转换为物理地址 + 当前进程 PID
          - 缓存控制器比较：  
              - 物理地址 Tag 是否匹配？
              - PID 是否匹配？
          - **只有两者均匹配**，才判定为缓存命中
    3. **进程切换时的行为**  
          - **无需刷新缓存**：新进程的 PID 不同，即使虚拟地址相同，也不会错误命中旧进程的数据
          - **PID 回收时刷新**：仅当 PID 被操作系统回收（如进程退出）时，才需清理对应 PID 的缓存行

操作系统和用户程序可能为同一物理地址使用两种不同的虚拟地址。这些重复地址称为同义地址或别名地址，可能会在虚拟缓存中生成同一数据的两个副本；如果其中一个被修改了，另一个就会包含错误值。而采用物理缓存是不可能发生这种情况的，因为这些访问将会首先被转换为相同的物理缓存块

页面着色（Page Coloring） 是一种通过控制物理页帧的分配策略，确保不同虚拟地址映射到同一物理地址时，其缓存索引（Index Bits）相同的技术

核心思想：

- 将物理内存页帧划分为若干“颜色”（Color），每个颜色对应一组特定的缓存索引位。
- 操作系统分配物理页时，强制让同一物理地址的虚拟页具有相同的“颜色”，从而避免别名冲突

!!! note "AI 解释"

    **(1) 缓存与页面的关系**
    
    假设：
    
    - 缓存大小为 \( C \)，组相联度为 \( N \)，每行大小 \( B \)。  
    - 缓存组数 \( S = \frac{C}{N \times B} \)，索引位数为 \( \log_2 S \)。  
    - 物理页大小 \( P \)（如 4KB），页内偏移位数为 \( \log_2 P \)。  
    
    **关键观察**：
    
    - 缓存的索引位（Index）通常来自虚拟地址的 **低位**（如页内偏移部分）
    - 若两个虚拟页的索引位相同，它们会映射到同一缓存组，可能引发别名
    
    **(2) 页面着色机制**
    
    1. **划分颜色**：  
          - 将物理页帧按缓存索引位的可能值划分为 \( K = 2^{\text{Index位数}} \) 种颜色（如 64 组缓存 → 64 种颜色）
          - 例如，若索引位为 6 位（\( 2^6 = 64 \) 组），则颜色编号 0~63 
    2. **分配策略**：  
          - 操作系统分配物理页时，根据进程的虚拟地址索引位 **强制选择对应颜色** 的物理页帧
          - 公式：$\text{物理页帧颜色} = (\text{虚拟地址索引位}) \mod K$
          - 这样，同一物理地址的所有虚拟映射会 **自动对齐到同一缓存组**，避免别名  
    
    **(3) 工作流程示例**
    
    - **场景**：  
          - 缓存：64 组（Index 6 位），页大小 4KB（偏移 12 位）
          - 虚拟地址 `VA1` 和 `VA2` 映射到同一物理页，但 `VA1` 的 Index=5，`VA2` 的 Index=10
    - **传统问题**：  
          - `VA1` 和 `VA2` 会分别映射到缓存组 5 和 10，导致同一数据存两份（别名） 
    - **页面着色解决**：  
          - 强制 `VA1` 和 `VA2` 使用颜色=5 的物理页（即物理页帧的 Index 位固定为 5）  
          - 访问时，无论 `VA1` 还是 `VA2`，均映射到缓存组 5，保证数据唯一性

#### 3.1.4 Trace caches

**to reduce hit time**

**trace caches 缩短命中时间**

**AI 解释**

Trace Cache 是一种用于优化处理器指令缓存性能的高级技术，主要通过在缓存中存储动态指令序列（即“执行轨迹”）来提升指令供给效率

1. 基本概念
      - 传统指令缓存（L1 ICache）：存储静态的指令代码（按内存地址顺序），但无法直接反映程序的实际执行路径（如分支后的指令流）
      - Trace Cache：存储的是处理器实际执行的 **动态指令序列**（称为“trace”），即按程序运行时顺序捕获的指令块（包含分支预测后的路径）
2. 工作原理
      - 动态轨迹记录：当处理器执行指令时，Trace Cache 会记录分支预测后的指令流，形成一个连续的指令块（trace）
          - 例如：循环或条件分支中的指令会被记录为一条线性化的轨迹，即使它们在内存中不连续
      - 索引方式：通过指令的起始地址（如分支指令的PC）和分支历史（全局或局部）作为索引，确保同一段代码在不同执行路径下生成不同的 trace
3. 关键优势
      - **减少取指延迟**：传统缓存需要多次访问（尤其在分支密集代码中），而 Trace Cache 直接提供预测后的指令流，避免重复解码和分支预测
      - **提高指令供给带宽**：每个trace包含多条指令（可能跨基本块），单次访问可获取更多有效指令
      - **缓解解码瓶颈**：存储已解码的微操作（uOps），避免重复解码（在NetBurst架构等中使用）

### 3.2 Increase cache bandwidth

#### 3.2.1 Pipelined Access

**to increase bandwidth**

**实现缓存访问的流水化，以提高缓存带宽**

**AI 解释**

缓存访问流水化是一种通过将缓存访问过程分解为多个流水级（pipeline stages），以提高缓存带宽和处理器整体性能的技术。其核心思想是 **将串行的缓存访问操作拆分成多个并行执行的阶段**，从而在每个时钟周期内处理更多的请求，提升吞吐量

**1.传统缓存访问的瓶颈**

在非流水化的缓存设计中，一次完整的缓存访问通常需要在一个时钟周期内完成以下操作：

1. 地址计算（Address Calculation）
2. 标签匹配（Tag Comparison）
3. 数据读取（Data Access）
4. 数据返回（Data Return）

由于这些步骤是串行执行的，如果缓存访问延迟较高（如大容量缓存或高频率设计），可能会导致：

1. 时钟周期变长（降低处理器频率）
2. 带宽受限（每个周期只能处理一个请求）

**2.缓存访问流水化的基本原理**

流水化缓存访问的核心思想是 **将缓存访问过程拆分成多个阶段**，类似于 CPU 流水线。典型的缓存流水化设计可能包括以下阶段：

| 流水级 | 操作 |
| :--: | :--: |
| Stage 1 | 地址计算（计算索引和标签）|
| Stage 2 | 标签匹配（比较标签是否命中）|
| Stage 3 | 数据读取（从数据阵列中读取数据）|
| Stage 4 | 数据返回（将数据送回 CPU 或下一级缓存）|

1. 每个时钟周期，一个新的缓存请求可以进入流水线的第一级
2. 前一个请求进入下一级流水线，同时新的请求进入第一级
3. 最终，每个时钟周期可以完成一个缓存访问（尽管单个访问仍然需要多个周期）

---

- 优势
      1. 提高缓存带宽
          - 传统缓存每个周期只能处理一个请求，而流水化缓存可以 每个周期接受一个新请求（即使单个访问仍需多个周期完成）
          - 适用于高吞吐量场景（如超标量处理器、多核共享缓存）
      2. 支持更高时钟频率：由于每个流水级逻辑更简单，可以缩短关键路径，提高处理器主频
      3. 提高多级缓存的效率：适用于 L1、L2、L3 缓存，特别是多端口缓存（如同时支持 load/store 请求）
- 挑战
      1. 访问延迟增加
          - 虽然吞吐量提高，但单个请求的延迟可能增加（因为需要多个时钟周期完成）
          - 需要通过预取（prefetching）或乱序执行（OoO）来隐藏延迟
      2. 流水线冲突
          - 如果多个请求访问同一缓存行，可能导致结构冲突（structural hazard）
          - 解决方法
              - 多端口缓存（支持多个并行访问）
              - banked 缓存（将缓存分成多个 bank，允许并行访问不同 bank）
      3. 一致性维护复杂：在多核处理器中，流水化缓存需要处理缓存一致性协议（如 MESI）带来的额外复杂度

#### 3.2.2 Nonblocking Cache

**to increase cache bandwidth**

**采用无阻塞缓存，以提高缓存带宽**

对于允许乱序执行的流水化计算机，其处理器不必因为一次数据缓存缺失而停顿。例如，在等待数据缓存返回缺失数据时，处理器可以继续从指令缓存中提取指令。无阻塞缓存（或称为自由查询缓存）允许数据缓存在一次缺失期间继续提供缓存命中，进一步加强了这种方案的潜在优势。这种“缺失时仍然命中”优化方法在缺失期间非常有用，它不会在此期间忽略处理器的请求，从而降低了实际缺失代价。还有一种精巧而复杂的选项:如果能够重看多个缺失,那缓存就能进一步降低实际缺失代价,这种选项被称为“多次缺失时仍然命中”（hitundermultiple miss）或者“缺失时缺失”（miss under miss）优化方法。只有当存储器系统可以为多次缺失提供服务时，第二种选项才有好处

<figure markdown="span">
    ![Img 12](../../../../img/comp_arch/ch3/ca_ch3_img12.png){ width="600" }
</figure>

#### 3.2.3 Multibanked Caches

**to increase cache bandwidth**

**采用多种缓存，以提高缓存带宽**

将缓存划分为几个相互独立、支持同时访问的缓存组，而不是将它们看作一个整体

显然，当访问请求很自然地分布在缓存组之间时，分组方式的效果最佳，所以将地址映射到缓存组的方式影响着存储器系统的行为。一种简单有效的映射方式是将缓存块地址按顺序分散在这些缓存组中，这种方式称为顺序交错。例如，如果有 4 个缓存组，0 号缓存组中的所有缓存块地址对 4 求模后余 0，1 号缓存组中的所有缓存块地址对 4 求模后余 1，以此类推

<figure markdown="span">
    ![Img 13](../../../../img/comp_arch/ch3/ca_ch3_img13.png){ width="600" }
</figure>

### 3.3 Reduce the miss penalty

#### 3.3.1 Multilevel Caches

**to reduce miss penalty**

**采用多级缓存降低缺失代价**

处理器与存储器之间的性能差距让架构师开始思考这样一个问题:是应当加快缓存速度以与处理器速度相匹配呢？还是让缓存更大一些，以避免加宽处理器与主存储器之间的鸿沟？

一个回答是：两者都要。在原缓存与存储器之间再添加一级缓存可以简化这一决定。第一级缓存可以小到足以与快速处理器的时钟周期时间相匹配。而第二级缓存则大到足以捕获本来可能进入主存储器的访问，从而降低实际缺失代价

尽管再添加一级层次结构的思路非常简单，但它增加了性能分析的复杂程度。第二级缓存的定义也并非总是那么简单。首先让我们为一个二级缓存定义存储器平均访问时间。用下标 L1 和 L2 分别指代第一级、第二级缓存，原公式为：

- $average\ memory\ access\ time = hit\ time_{L1} + miss\ rate_{L1} \times miss\ penalty_{L1}$
- $miss\ penalty_{L1} = hit\ time_{L2} + miss\ rate_{L2} \times miss\ penalty_{L2}$

$\Rightarrow$

$average\ memory\ access\ time = hit\ time_{L1} + miss\ rate_{L1} \times (hit\ time_{L2} + miss\ rate_{L2} \times miss\ penalty_{L2})$

在这个公式中，第二级缺失率是针对第一级缓存未能找到的内容进行测量的。对二级缓存系统采用以下术语

1. local miss rate（局部缺失率）：$\dfrac{缓存中的缺失数}{对该缓存进行存储器访问的总数}$，对于第一级缓存，等于 $miss\ penalty_{L1}$；对于第二级缓存，等于 $miss\ penalty_{L2}$
2. global miss rate（全局缺失率）：$\dfrac{缓存中的缺失数}{处理器生成的存储器访问总数}$，对于第一级缓存，等于 $miss\ penalty_{L1}$；对于第二级缓存，等于 $miss\ penalty_{L1} \times miss\ penalty_{L2}$

在评估第二级缓存时，应当使用全局缓存缺失率

$average\ memory\ stalls\ per\ instruction = misses\ per\ instruction_{L1} \times hit\ time_{L2} + misses\ per\ instruction_{L2} \times miss\ penalty_{L2}$

<figure markdown="span">
    ![Img 14](../../../../img/comp_arch/ch3/ca_ch3_img14.png){ width="600" }
</figure>

#### 3.3.2 Critical Word First and Early Restart

**to reduce miss penalty**

**关键字优先和提前重启动以降低缺失代价**

这一技术的事实基础是人们观测到处理器在某一时刻通常仅需要缓存块的一个字。这一策略显得“不够耐心”：无须等待完成整个块的载入过程，一旦载入被请求字之后，立即将其发出然后就重启处理器。下面是两个特定策略

1. 关键字优先：首先从存储器中请求缺失的字，在其到达缓存之后立即发给处理器;使处理器能够在载入块中其他字时继续执行
2. 提前重启动：以正常顺序提取字，但只要块中的被请求字到达缓存，就立即将其发送给处理器，让处理器继续执行

通常，只有在采用大型缓存块的设计中，这些技术才有用武之地，如果缓存块很小，它们带来的好处是很低的。注意，在载入某个块中的其余内容时，缓存通常可以继续满足对其他块的访问请求

**AI 解释**

在缓存（Cache）设计中，缺失代价（Miss Penalty）是指处理器因缓存未命中（Cache Miss）而不得不从下级存储（如主存或更高级缓存）加载数据所导致的额外延迟。为了减少缺失代价，现代处理器采用了多种优化技术，其中关键字优先（Critical Word First, CWF）和 提前重启动（Early Restart）是两种重要的方法。它们的核心思想是尽早提供处理器所需的关键数据，而不是等待整个缓存行完全加载

- 缓存行（Cache Line）：缓存的最小存储单元，通常为 64 字节（现代 x86 CPU）
- 缓存缺失（Cache Miss）：当处理器访问的数据不在当前缓存中时，需要从下级存储（如主存或L2缓存）加载整个缓存行
- 缺失代价（Miss Penalty）：加载整个缓存行所需的时间，通常几十到几百个时钟周期

那么问题来了：如果处理器只需要缓存行中的某个字（如 4 字节），为什么必须等待整个 64 字节加载完成呢？

**关键字优先**（CWF）

1. 基本思想
      - 当缓存未命中时，优先加载并返回处理器当前请求的数据（关键字，Critical Word），然后再加载缓存行的其余部分
      - 这样可以让处理器尽早继续执行，而不必等待整个缓存行加载完成
2. 工作流程
      1. 缓存未命中：处理器请求某个字（如 0xABCD1234），但该字不在当前缓存中
      2. 请求下级存储：缓存控制器向主存或下级缓存发送请求，并指定关键字的地址
      3. 关键字优先返回
          - 主存或下级缓存先返回关键字（0xABCD1234），同时后台继续传输缓存行的剩余部分
          - 处理器在收到关键字后立即恢复执行，而不必等待整个缓存行加载完成
      4. 填充剩余数据：缓存行的其余部分稍后加载并存入缓存
3. 优势
      - 显著降低有效缺失代价：处理器只需等待关键字的加载时间，而不是整个缓存行的加载时间
      - 适用于顺序访问模式：如果后续访问的数据仍在传输中，可以利用缓存带宽优化
4. 适用场景
      - 单次访问（如指针解引用）：只需一个关键数据，其余数据可能暂时不需要
      - 乱序执行（OoO）：允许处理器在数据到达后立即继续执行，提高指令级并行度（ILP）

**提前重启动**

1. 基本思想
      - 类似于关键字优先，但不严格区分关键字，而是一旦缓存行的部分数据到达，就立即提供给处理器
      - 适用于顺序访问模式（如数组遍历），处理器可能很快会访问后续数据
2. 工作流程
      1. 缓存未命中：处理器请求某个数据，但缓存未命中
      2. 从下级存储加载：缓存控制器开始加载整个缓存行
      3. 部分数据到达时提前返回：
          - 一旦请求的数据块（可能不是严格的关键字，而是缓存行的一部分）到达，就立即返回给处理器
          - 同时，后台继续加载剩余数据
      4. 处理器继续执行：无需等待整个缓存行加载完成
3. 优势
      - 适用于顺序访问：如果程序按顺序访问数据（如数组遍历），提前返回的数据可能正好是下一个所需数据
      - 减少平均访问延迟：比关键字优先更灵活，适用于连续访问模式

| 特性 | 关键字优先 | 提前重启动 |
| :--: | :--: | :--: |
| 数据返回策略 | 仅优先返回请求的关键字 | 返回任意先到达的部分数据 |
| 适用场景 | 单词随机访问（如指针访问）| 顺序访问（如数组遍历）|
| 实现复杂度 | 需要识别关键字地址 | 只需检测数据是否到达 |

#### 3.3.3 Giving Priority to Read Misses over Writes

**to reduce miss penalty**

**使读取缺失的优先级高于写入缺失，以降低缺失代价**

若给定一个直接映射 + 直写的 cache，现有一指令序列：

```verilog linenums="1"
SM R3, 512(R0)    ; 存储：M[512] ← R3（缓存索引 0）
LM R1, 1024(R0)   ; 加载：R1 ← M[1024]（缓存索引 0）
LM R2, 512(R0)    ; 加载：R2 ← M[512]（缓存索引 0）
```

1. `SM R3, 512(R0)`
      1. 将 R3 的值写入内存地址 512
      2. 直写缓存：数据同时写入缓存和主存，但写入主存可能被缓冲到写入缓冲区（Write Buffer）
      3. 此时写入缓冲区中有一个待完成的写操作（512 → R3）
2. `LM R1, 1024(R0)`
      1. 尝试从 1024 加载数据到 R1
      2. 1024 和 512 映射到同一缓存块（索引 0），导致冲突缺失（Conflict Miss）
      3. 读取缺失时，缓存不会检查写入缓冲区，因此直接从主存读取 1024 的值
      4. 缓存块被替换为 1024 的数据，写入缓冲区中的 512 的写操作可能尚未完成
3. `LM R2, 512(R0)`
      1. 尝试从 512 加载数据到 R2
      2. 由于缓存块当前存储的是 1024 的数据，再次发生缺失
      3. 缓存从主存读取 512 的值：
          1. 如果写入缓冲区尚未将 R3 的值写入主存，则读到的是 旧值（而非 R3 的值）
          2. 因此 R2 ≠ R3

摆脱这一两难境地的最简单方法是让读取缺失一直等待到写人缓冲区为空为止。一种替代方法是在 ==发生读取缺失时检查写入缓冲区的内容，如果没有冲突而且存储器系统可用，则让读取缺失继续==。几乎所有桌面与服务器处理器都使用后一方法，使读取操作的优先级高于写入操作

处理器在 **写回** 缓存中的写入成本也可以降低。假定一次读取缺失将替换一个脏服务器块。我们不是将这个脏块写到存储器中，然后再读取存储器，而是将这个脏块复制到缓冲区中，然后读存储器，然后再写存储器。这样，处理器的读取操作将会很快结束(处理器可能正在等待这一操作的完成)。和前一种情况类似，如果发生了读取缺失，处理器或者停顿到缓冲区为空或者检查缓冲区中各个字的地址，以了解是否存在冲突

!!! tip "AI 解释"

    **传统写回缓存的读取缺失处理**

    在标准写回缓存中，当发生 读取缺失（Read Miss） 且需要替换一个 脏块（Dirty Block，即已被修改但未写回主存的缓存行） 时，流程如下：

    1. 写回脏块：先将脏块写回主存（高延迟操作）
    2. 读取新块：再从主存加载新数据到缓存
    3. 处理器停顿：整个过程中，处理器必须等待这两步完成才能继续执行

    ---

    **优化方法：脏块缓冲（Dirty Block Buffering）**

    为了减少处理器等待时间，提出以下改进方案：

    1. 脏块复制到缓冲区：
          1. 不立即将脏块写回主存，而是先复制到临时缓冲区
          2. 同时立即发起对新数据的读取（从主存加载到缓存）
    2. 异步写回脏块：在主存读取完成后，后台将缓冲区中的脏块写回主存
    3. 处理器无需完全停顿：处理器在新数据加载到缓存后即可继续执行，无需等待脏块写回完成

#### 3.3.4 Merging Write Buffer

**to reduce miss penalty**

**合并写缓冲区以降低缺失代价**

因为所有存储内容都必须发送到层次结构的下一层级，所以直写缓存依赖于写缓冲区。即使是写回缓存，在替代一个块时也会使用一个简单的缓冲区。如果写缓冲区为空，则数据和整个地址被写到缓冲区中，从处理器的角度来看，写入操作已经完成;在写缓冲区准备将字写到存储器时，处理器继续自己的工作。如果缓冲区中包含其他经过修改的块，则检查它们的地址，看看新数据的地址是否与写缓冲区中有效项目的地址匹配。如果匹配，则将新数据与这个项目合并在一起。这种优化方法称为写合并

如果缓冲区已满，而且没有匹配地址，则缓存（和处理器）必须等待，直到缓冲区中拥有空白项目为止。由于多字写入的速度通常快于每次只写入一个字的写入操作，所以这种优化方法可以更高效地使用存储器

这种优化方式还会减少因为写缓冲区已满而导致的停顿。下图显示了一个写缓冲区在采用和不采用写合并时的情况。假定这个写缓冲区中有四项，每一项有 4 个 64 位的字。在采用这一优化方法时，图中的 4 个字可以完全合并，放在写缓冲区的一个项目中，而在不采用这优化方法时，对写缓冲区的连续地址执行 4 次存储操作，将整个缓冲区填满，每个项目中保存一个字

<figure markdown="span">
    ![Img 15](../../../../img/comp_arch/ch3/ca_ch3_img15.png){ width="600" }
</figure>

注意，输入/输出设备寄存器经常被映射到物理地址空间。由于 I/O 寄存器是分享的，不能像存储器中的字数组那样操作，所以这些地址不能允许写合并。例如，它们可能要求为每个 I/O 寄存器提供一个地址和一个数据字，而不能只提供一个地址进行多字写入。为了实现这些副作用，通常由缓存将这些页面进行标记，表明其需要采用非合并直写方式

!!! tip "AI 解释"

    **问题背景**

    如果多次写入同一缓存行（或相邻地址），每次单独写入主存效率低下。例如：连续写入 A[0]、A[1]、A[2]，若分 3 次写主存，带宽利用率低
    
    目标：合并多次写入，减少主存访问次数

    ---

    **工作流程**

    1. 写入缓冲区非满
          1. 处理器写入数据到写缓冲区，立即继续执行
          2. 如果新写入的地址与缓冲区中某条目地址连续/属于同一块，则合并数据（而非新增条目）。例如：缓冲区已有 `[A0, A1]`，新写入 `A2` → 合并为 `[A0, A1, A2]`。若写入 `B0`（不连续），则新增条目
    2. 写入缓冲区满
          1. 若缓冲区已满且无法合并，处理器必须等待缓冲区腾出空间
          2. 主存控制器会批量写入合并后的数据（如一次写入 64 字节而非多次写入 4 字节）

#### 3.3.5 Victim Caches

**受害者缓存**

Victim Cache 是一种辅助缓存结构，用于减少冲突缺失（Conflict Misses）和提高缓存命中率，尤其在直接映射（Direct-Mapped）或低相联度（Low Associativity）缓存中效果显著。其核心思想是临时保存被替换出的缓存行，为后续访问提供二次机会

基本工作原理：

1. 主缓存（如 L1）在发生缓存替换时，被替换出的缓存行（称为“受害者”）不直接丢弃，而是暂存到 Victim Cache 中
2. 当主缓存未命中时，优先检查 Victim Cache：
      1. 若命中，则将数据移回主缓存，避免访问下级存储（如 L2 或主存）
      2. 若未命中，再访问下级存储

!!! question "为什么需要 Victim Cache"

    1. 直接映射缓存的缺陷：同一索引的多个地址会互相冲突（如地址 A 和 B 映射到同一缓存行，交替访问会导致频繁替换）
    2. 低相联度缓存的局限性：组相联缓存（如 2-way/4-way）仍可能因冲突导致缺失
    3. Victim Cache 的作用：作为“逃生舱”，保存最近被替换的块，缓解冲突缺失

典型架构：

1. 位置：通常位于 L1 缓存和 L2 缓存之间（或集成在L1中）
2. 容量：较小（如 4-16 条目），全相联（Fully Associative）设计，避免额外索引开销
3. 替换策略：LRU（最近最少使用）或 FIFO

### 3.4 Reduce the miss rate

#### 3.4.1 Where misses come from

1. compulsory miss（强制性缺失）：数据首次被访问时，必然不在缓存中
2. capacity miss（容量缺失）：缓存容量不足，无法容纳所有活跃数据块
3. conflict miss（冲突缺失）：多个地址映射到同一缓存组（如直接映射缓存中地址 A 和 B 索引相同）
4. coherence miss（一致性缺失）：多核系统中，其他核心修改共享数据导致本地缓存块失效

我们无法很大程度上减少 capacity miss 除非把 cache 做的很大。但是我们可以通过很多种方法减少 conflict misses 和 compulsory misses

1. 增大 block size：
      1. 减少 compulsory miss
      2. 增加 capacity miss：固定缓存容量，块越大，总块数越少
2. 增大 cache size：
      1. 显著减少 capacity miss
      2. 间接减少 conflict miss
3. 提高 associativity：
      1. 轻微减少 capacity miss
      2. 显著减少 conflict miss

#### 3.4.2 Larger Block Size

**to reduce miss rate**

**增大块大小以降低缺失率**

通过充分利用 spatial locality 降低 compulsory miss

较大的块也会增加 miss penalty。固定缓存容量，块越大，总块数越少，因此可能会增加 conflict miss

如果缓存很小，还会增加 capacity miss

<figure markdown="span">
    ![Img 16](../../../../img/comp_arch/ch3/ca_ch3_img16.png){ width="600" }
</figure>

<figure markdown="span">
    ![Img 17](../../../../img/comp_arch/ch3/ca_ch3_img17.png){ width="600" }
</figure>

<figure markdown="span">
    ![Img 18](../../../../img/comp_arch/ch3/ca_ch3_img18.png){ width="600" }
</figure>

#### 3.4.3 Larger Caches

**to reduce miss rate**

**增大缓存以降低缺失率**

降低 capacity miss

但会增加命中时间、增加成本和功耗

#### 3.4.4 Higher Associativity

**to reduce miss rate**

**提高相联度以降低缺失率**

显著减少 conflict miss

2:1 cache rule of thumb（缓存经验规律）：大小为 $N$ 的直接映射缓存与大小为 $\dfrac{N}{2}$ 的两路组相联缓存具有大体相同的缺失率

<figure markdown="span">
    ![Img 19](../../../../img/comp_arch/ch3/ca_ch3_img19.png){ width="600" }
</figure>

增大相联度可能会延长命中时间

<figure markdown="span">
    ![Img 20](../../../../img/comp_arch/ch3/ca_ch3_img20.png){ width="600" }
</figure>

#### 3.4.5 Way Prediction and Pseudo-Associative Cache

**路预测与伪相联缓存**

路预测：

1. 减少 conflict miss：预测成功时，快速命中目标数据，避免因串行比较导致的流水线停顿
2. 间接优化 capacity miss：更快的访问允许缓存更高效地服务请求，减少因延迟导致的替换

---

Pseudo-Associative Cache（伪相联缓存）：

核心思想：

1. 结合直接映射缓存（Direct-Mapped） 的低延迟和 组相联缓存 的低冲突缺失特性
2. 首次访问按直接映射处理，若缺失则触发“二次查找”（类似组相联）

工作原理：

1. 首次访问（直接映射）：
      1. 数据仅映射到唯一位置（如 Index = Address % Cache_Size）
      2. 若命中，访问结束（延迟与直接映射相同）
2. 首次缺失时二次查找（组相联）：
      1. 检查同一 Set 的另一位置（如通过哈希函数生成备用 Index）
      2. 若二次查找命中，数据被返回并可能交换到主位置（优化后续访问）
3. 替换策略：二次查找仍缺失时，按 LRU 等策略替换

对缺失率的影响：

1. 显著减少 conflict miss：直接映射的冲突可通过二次查找缓解（类似 2 路组相联）
2. 保持低延迟：多数情况下（首次命中）维持直接映射的速度

#### 3.4.6 Compiler Optimizations

**to reduce miss rate**

**采用编译器优化以降低缺失率**

##### Merging Arrays

**合并数组**

合并数组指的是将多个独立的数组合并为一个复合数组，或者将多个数据结构字段重新组织为数组结构

提高 spatial locality

**1.Array Packing**（数组打包）

```c linenums="1"
// 合并前
float x[N], y[N], z[N];

// 合并后
struct Point { float x, y, z; };
Point points[N];
```

**2.结构体数组转换**（AoS → SoA）

将"结构体数组"（Array of Structures）转换为"数组结构体"（Structure of Arrays）

```c linenums="1"
// AoS 形式（Array of Structures）
struct Particle {
    float x, y, z;
    float vx, vy, vz;
};
Particle particles[N];

// SoA 形式（Structure of Arrays）
struct Particles {
    float x[N], y[N], z[N];
    float vx[N], vy[N], vz[N];
};
```

##### Loop Interchange

**循环交换**

一些程序中存在嵌套循环，它们会以非连续顺序访问存储器中的数据。只要交换一下这些循环的嵌套顺序，就可能使程序代码按照数据的存储顺序来访问它们。如果缓存中无法容纳这些数组，这一技术可以通过提高空间局域性来减少缺失；通过重新排序，可以使缓存块中的数据在被替换之前，得到最大限度的充分利用。例如，设x是一个大小为 [5000, 100] 的两维数据，其分配方式使得 `x[1, j]` 和 `x[i, j + 1]` 相邻(由于这个数组是按行进行排列的，所以我们说这种顺序是以行为主的)，以下两段代码说明可以怎样来优化访问过程:

<div class="grid" markdown>

```c linenums="1" title="优化前"
for (j = 0; j < 100; j++) {
    for (i = 0; i < 5000; i++) {
        x[i][j] = 2 * x[i][j];
    }
}
```

```c linenums="1" title="优化后"
for (i = 0; i < 5000; i++) {
    for (j = 0; j < 100; j++) {
        x[i][j] = 2 * x[i][j];
    }
}
```

</div>

原代码以 100 个字的步幅跳跃式浏览存储器，而修改后的版本在访问了个缓存块中的所有字之后才进人下一个块。这一优化方法提高了缓存性能，却没有影响到所执行的指令数目

##### Loop Fusion

**循环融合**

循环融合是一种编译器优化技术，它将多个相邻的、具有相同迭代空间的循环合并为单个循环，以减少循环开销和提高数据局部性

<div class="grid" markdown>

```c linenums="1" title="融合前"
// 第一个循环
for (int i = 0; i < N; i++) {
    a[i] = b[i] + c[i];
}

// 第二个循环
for (int i = 0; i < N; i++) {
    d[i] = a[i] * e[i];
}
```

```c linenums="1" title="融合后"
for (int i = 0; i < N; i++) {
    a[i] = b[i] + c[i];
    d[i] = a[i] * e[i];
}
```

</div>

##### Blocking

**分块**

分块技术将大型数据结构或循环迭代空间划分为较小的块（tile），使得每个块能够完全装入 CPU 缓存中。这样可以在处理完一个块的所有数据后，再移动到下一个块，而不是按照原始顺序遍历整个数据结构

当程序访问的数据无法在缓存中找到（缓存未命中）时，必须从主内存加载，这会显著降低性能。分块技术通过确保数据在被重复使用前保留在缓存中，减少了缓存未命中的次数

矩阵乘法：

```c linenums="1" title="无分块"
for (i = 0; i < N; i++) {
    for (j = 0; j < N; j++) {
        for (k = 0; k < N; k++) {
            C[i][j] += A[i][k] * B[k][j];
        }
    }
}
```

```c linenums="1" title="分块优化"
#define BLOCK_SIZE 32

for (ii = 0; ii < N; ii += BLOCK_SIZE) {
    for (jj = 0; jj < N; jj += BLOCK_SIZE) {
        for (kk = 0; kk < N; kk += BLOCK_SIZE) {
            // 处理一个块
            for (i = ii; i < ii + BLOCK_SIZE; i++) {
                for (j = jj; j < jj + BLOCK_SIZE; j++) {
                    for (k = kk; k < kk + BLOCK_SIZE; k++) {
                        C[i][j] += A[i][k] * B[k][j];
                    }
                }
            }
        }
    }
}
```

### 3.5 Reduce the miss penalty and miss rate via parallelism

#### 3.5.1 Hardware Prefetching of Instructions and Data

**to reduce miss penalty or miss rate**

**对指令和数据进行硬件预取，以降低缺失代价或缺失率**

处理器通过监测当前的内存访问模式，预测接下来可能会被访问的内存地址，并在这些数据被实际请求之前，自动将它们从主存预取到缓存中

硬件预取是一种空间换时间的优化

1. 降低 miss penalty
      - 传统情况：当发生缓存缺失时，处理器必须等待数据从慢速的主存加载，造成流水线停顿（stall）
      - 使用预取：所需数据已经被提前加载到缓存中，缺失时可直接从缓存获取，避免等待主存访问
2. 降低 miss rate
      - 传统情况：首次访问新数据必然导致缓存缺失
      - 使用预取：在程序实际需要数据前已经将其加载到缓存，使得"首次访问"变成缓存命中

预取技术依赖于存在可用的额外内存带宽，且不会因占用带宽造成性能损失

#### 3.5.2 Compiler-Controlled Prefetching 

**to reduce miss penalty or miss rate**

**用编译器控制预取，以降低缺失代价或缺失率**

编译器通过分析程序的内存访问模式，在适当位置插入预取指令，提前获取数据，在实际使用数据前启动加载操作

- register prefetch：将数据加载到寄存器
- cache prefetch：将数据加载到缓存

两种类型：

1. 绑定预取（binding prefetch）：类似普通加载指令，但需严格保证地址有效性，否则会触发异常
2. 非绑定预取（non-binding prefetch）：安全推测，即使地址无效也不会报错

不过，发出预取指令会导致指令开销，所以编译器必须非常小心地确保这些开销不会超过所得到的好处。如果程序能够将注意力放在那些可能导致缓存缺失的引用上，就可以避免不必要的预取操作，同时明显缩短存储器平均访问时间

<figure markdown="span">
    ![Img 21](../../../../img/comp_arch/ch3/ca_ch3_img21.png){ width="600" }
</figure>

---

<figure markdown="span">
    ![Img 22](../../../../img/comp_arch/ch3/ca_ch3_img22.png){ width="600" }
</figure>
