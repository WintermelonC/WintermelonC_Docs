# 12 大语言模型与生成式人工智能

<!-- !!! tip "说明"

    本文档正在更新中…… -->

!!! info "说明"

    本文档仅涉及部分内容，仅可用于复习重点知识

## 12.1 概述

### 12.1.1 AIGC 与 LLM

AIGC（Artificial Intelligence Generated Content，人工智能生成内容），是指利用人工智能技术，使计算机自动生成各种形式的内容，如文章、音乐、图片、视频等。 AIGC 与以前的搜索引擎技术不同，搜索引擎只能获取已经存在的内容或者知识，如某个网页、数据库中的某段文字。而 AIGC 则可以根据输入或提示，模拟生成相应的新内容

1. AIGC 那以实现原创性：作为原有内容的一种排列组合，难以实现原创性
2. AIGC 缺乏可解释性：基于概率来预测新的内容 （自回归算法），缺乏可解释性

LLM（Large Language Model，大语言模型），是专门用于执行 NLP 任务的语言模型

1. 训练数据大
2. 参数规模大
3. 训练耗资大

涌现能力：当一种系统在复杂性增加到某一临界点时，会出现其子系统或较小规模版本中未曾存在的行为或特性。在大模型中将这种能力称为涌现能力，这些能力通常是训练目标未明确设计但模型能够自动学会的技能

需要注意的问题：

1. 模型不是越大越好，尤其是在专业领域
2. 数据偏差与偏见依然存在
3. 隐私和数据保护依然是关键

### 12.1.2 GAI 与 AGI

GAI（Generative Artificial Intelligence，生成式人工智能），是特指能生成全新内容的 AI，其生成的内容就是前面所提的 AIGC。AIGC 侧重内容生成的来源，而 GAI 侧重 AI 系统的功能特点

1. 自回归生成技术，带有随机性
2. 不是搜索引擎

AGI（Artificial General Intelligence ，通用人工智能），是指机器能够完成人类能够完成的任何智力任务的能力。它旨在实现一般的认知能力，能够适应任何情况或目标，是人工智能研究的最终目标之一。 AGI 能够执行各种复杂的任务，包括学习、计划、解决问题、抽象思维、理解复杂理念等

AGI 要求在领域无关、任务无关的环境下进行训练，给模型投喂的数据集要包括多模态的数据，即文本、语音、视频、图像、气味等。该模型应具备各种能力

### 12.1.3 GPT 与 ChatGPT

GPT（Generative Pre trained Transformer，生成式预训练变换器）是一种基于 Transformer 结构的预训练模型

ChatGPT（Chat Generative Pre trained Transformer），是 OpenAI 推出的采用 GPT 架构的聊天机器人

### 12.1.4 预训练与微调

预训练（海量，无监督，目标是通用能力）：不针对特定任务，而是用大规模的数据训练模型，使其具备各方面的基础能力，类似于我们人类的通识教育

微调（小量，有监督，目标是任务适配）：利用预训练模型已经学到的通用特征表示，针对特定任务微调使模型能够更好地适应某个任务

### 12.1.5 AIGC 的造假甄别

1. 不可预知
2. 隐蔽
3. 难以辨别

## 12.2 LLM 微调数据集获取

### 12.2.1 中文开源数据集

1. 自然语言处理（NLP）
2. 计算机视觉
3. 财经领域
4. 通用数据集平台
5. 其他资源
